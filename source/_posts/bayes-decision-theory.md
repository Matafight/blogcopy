title: "Bayes decision theory"
date: 2015-04-02 10:49:36
categories: 机器学习
tags: bayes
---
**贝叶斯决策论**
贝叶斯决策论是解决模式分类问题的一种基本统计途径,其出发点是利用**概率**的不同分类决策与相应的**决策代价**之间的定量折中.它做出了如下假设:决策问题可以用概率的形式来描述,并且假设所有有关的概率结构均已知,其实它只是基于常识判别的一种形式化而已.

贝叶斯决策的核心就是*贝叶斯公式*:

p(w_i|x)=(p(x|w_i)*p(w_i))/p(x);

先对未知的类别假设一个先验概率,当没有训练数据的时候,此时新来一个观测值,只能根据先验概率来判断,但是当有训练数据的时候,这个数据是怎样影响判别结果的呢?通过贝叶斯公式中的似然函数,likehood P(x|w_i) ,一般也称为类条件概率,似然是指在其他条件相同的情况下,使p(x|w_i)最大的wi更有可能是真实的类别. 分母p(x)称为evidence ,仅仅是一个标量因子,以保证各类后验概率的总和为1.

判别时应当要使误差概率最小化.对于二类分类问题有
![](http://7xiegr.com1.z0.glb.clouddn.com/抓图2.png)
即如果p(w1|x)>p(w2|x) .则为w1 反之则为 w2,这是很直观的结论,也可以通过平均误差概率来证明;


要想使p(error)尽可能小,就要使p(error|x)尽可能小,由此我们验证了最小化误差概率下的贝叶斯决策,p(w1|x)>p(w2|x)就选w1.该式可以转化为后验概率的形式.

**分类器,判别函数及判定面**
有多种方式来表述模式分类器,其中用的最多的是一种判别函数,g_i(X),i=1....c 的形式,如果对于所有的j !=i  有 g_i(X)>g_j(X),则此分类器将特征向量X判给w_i
这里的g_i(X)是判别函数,一般 g_i(X)=-R(a_i|X),最大化判别函数就是最小化风险函数,当![](http://7xiegr.com1.z0.glb.clouddn.com/抓图3.png)的时候,最小化条件风险就是最小化误差概率.

对于两类分类情况,正常来说应当有两个判别函数,当g1(X)>g2(X)时判给第一类,但是,可以只使用一个判别函数g(X)=g1(X)-g2(X),可以通过判断g(X)>0 来判断属于哪一类,常用的二类分类器都是基于这个判别函数,如svm,perceptron等.










